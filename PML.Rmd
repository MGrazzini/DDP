---
title: "Coursera's Practical Machine Learning Peer Assessment"
author: "Matteo Grazzini"
date: "20 aprile 2015"
output: html_document
---

**Summary** 

The goal of the project is to predict the manner in which six people did a barbell lift exercise based on data from accelerometers on their belt, forearm, arm, and dumbell collected during different executions of the same exercise. 

**Analysis**

1. Loading necessary libraries for regression training:

```{r}
library(caret)
library(randomForest)
```

2. Loading testing and training dataset. Many variables hold constant NA value; after comparison I noticed that they are the same in both datasets and left them out. In addition, I removed variables not directly related to measurements:

``` {r}
testing <- read.csv("pml-testing.csv", na.strings = c("", "NA"))
training <- read.csv("pml-training.csv", na.strings = c("", "NA"))
testing2 <- testing[, c(2:11, 37:49, 60:68, 84:86, 102, 113:124, 140, 151:160)]
training2 <- training[, c(2:11, 37:49, 60:68, 84:86, 102, 113:124, 140, 151:160)]
testingf <- testing2[, 7:58] 
trainingf <- training2[, 7:59]
```

3. Creating data partition. I divided the training dataset into a proper training (70% of the rows) and a cross-validation one (30% of the rows).

```{r}
inTrain <- createDataPartition(y = trainingf$classe, p=0.7, list=FALSE)
training_def <- trainingf[ inTrain, ]
testing_def <- trainingf[ -inTrain, ]
```

4. Model fitting: I used a random forest algorithm on the partitioned training set combined with a k = 4 fold crossing validation
 
```{r eval = FALSE}
set.seed(3)
modFit <- train(classe ~., data = training_def, method = "rf", prox = TRUE, 
                trControl = trainControl(method = "cv", number = 4))
modFit
```

```
## Random Forest 
## 
## 13737 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
##  
##  No pre-processing
##  Resampling: Cross-Validated (4 fold) 
##  
##  Summary of sample sizes: 10302, 10303, 10303, 10303 
##  
##  Resampling results across tuning parameters:
##  
##    mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
##     2    0.9891533  0.9862780  0.001174663  0.001485094
##    27    0.9899540  0.9872916  0.002160481  0.002732999
##    52    0.9816552  0.9767897  0.002449609  0.003100451
##  
##  Accuracy was used to select the optimal model using  the largest value.
##  The final value used for the model was mtry = 27. 
```

5. Calculating the out of sample accuracy: using prediction on the partitioned testing dataset, accuracy of the random forest model turns out to be around 99,30%.

```{r eval = FALSE}
test_predict <- predict(modFit, testing_def)
confusionMatrix(test_predict, testing_def$classe)
```

```
##            Reference
##  Prediction    A    B    C    D    E
##           A 1671   10    0    0    0
##           B    2 1126    9    0    1
##           C    1    2 1012    8    0
##           D    0    1    5  955    2
##           E    0    0    0    1 1079
##  
##  Overall Statistics
##                                            
##                 Accuracy : 0.9929          
##                   95% CI : (0.9904, 0.9949)
##      No Information Rate : 0.2845          
##      P-Value [Acc > NIR] : < 2.2e-16       
##                                            
##                    Kappa : 0.991           
##   Mcnemar's Test P-Value : NA              
##  
##  Statistics by Class:
##  
##                       Class: A Class: B Class: C Class: D Class: E
##  Sensitivity            0.9982   0.9886   0.9864   0.9907   0.9972
##  Specificity            0.9976   0.9975   0.9977   0.9984   0.9998
##  Pos Pred Value         0.9941   0.9895   0.9892   0.9917   0.9991
##  Neg Pred Value         0.9993   0.9973   0.9971   0.9982   0.9994
##  Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
##  Detection Rate         0.2839   0.1913   0.1720   0.1623   0.1833
##  Detection Prevalence   0.2856   0.1934   0.1738   0.1636   0.1835
##  Balanced Accuracy      0.9979   0.9930   0.9920   0.9945   0.9985
```

6. Predicting values for initial testing dataset: I applied the random forest model to the initial testing dataset. Given the accuracy of the model all answers proved to be correct.

```{r eval = FALSE}
final_testing <- predict(modFit, testingf)
final_testing
```

```
## [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E
```